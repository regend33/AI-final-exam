# AI Part 7-6

## 개념적 군집화
군집화는 무엇을 기준으로 수행하느냐에 따라 서로 다른 군집을 생성하게 된다.

개념적 군집화는 구성원이 속한 환경이나 문맥에 관계 없이(Context-Free) 유사도가 결정되는 일반 군집화 기법과 달리 유사도 측정 시 구성원간의 상호 관계에 따라 유사도가 결정된다.

군집 간 유사도를 측정하는 방법

단일연결
- 두 군집간의 유사도가 가장 가까운 구성원 사이의 거리를 이용하는 방법

완전연결
- 두 군집간의 유사도가 가장 먼 구성원 사이의 거리를 이용하는 방법

집단평균
- 모든 구성원간의 유사도 평균을 이용하는 방법

## 강화 학습
학습자(Agent)와 환경(Environment)사이에 일어나는 상호작용을 이용하여 가장 큰(문제해결에 가장 효과가 좋은) 포상을 받을 수 있는 해결책을 찾는 방식으로 이를 강화 학습(Reinforcement Learning)이라고 한다. 강화 학습은 경험을 통해 학습한다는 점에서 지금까지 설명한 학습 방법과 다르고, 보다 더 까다로운 환경에서 수행되는 학습 방법이다.

강화 학습은 주어진 모델 없이 학습을 시작하고 학습하는데 필요한 예제도 제공되지 않는다. 또한 평가함수 같은 유틸리티도 스스로 만들어 가면서 학습을 수행하여야한다.

강화 학습은 학습자가 환경을 관측(Observation)하여 행동(Action)을 취하고 비평(Critic)을 통한 포상(Reward)을 받고 적절한 정책에 따라 다음 행동을 결정하는 과정을 반복하면서 수행된다.

여기서 다음 행동을 결정한다는 것은 계획수립(Planning)과 유사하지만 예상되는 포상에 대해 알지 못한다는 점에서 다르다.

강화 학습에서는 탐사(Exploration)와 개척(Exploitation) 작업이 필요하다. 탐사는 차후의 보다 더 나은 행동을 위하여 지식베이스를 확층하는 것이고, 개척은 현재의 지식(상태)만을 기반으로 가장 큰 포상을 받을 수 있는 행동을 취하는 것이다. 

이러한 탐사나 개척은 연기된 포상(Delayed Reward), 일반화(Generalization) 등과 함께 강화 학습에서 다루어야 할 중요한 문제들이다. 

연기된 포상 문제는 현재의 상태에서 어떤 행동(A1)을 취하여 최대의 포상을 받는 것 보다, 현재는 포상이 적더라도 다른 행동(A2)을 취하여 차후(여러 행동 수행 후)의 포상이 더 커질 수 있기 때문에 중요하다.

예를 들어, 장기 게임을 할 때 당장 졸이 몇 개 잡히는 것보다 현재 졸을 희생하고 나중에 상대의 왕을 잡아 게임을 이기는 것이 더 중요한 것이다. 일반화는 연습 데이터(Training Data)보다 평가 데이터(Test Data)를 가지고 더 좋은 결과를 보이기 위해 필요한 작업이다.

강화 학습에 필요한 요소
- t: 문제 해결 과정에서의 이산적(Discrete) 시간
- St: 시간 t에서 문제의 상태
- At: 시간 t에서의 포상(Reward)
- Rt: 한 상태에서 행동을 취하기 위한 정책
- V: 상태 값을 구하는 함수

정책은 어떤 시간에 취해야 되는 행동을 결정하는 것으로 생성규칙(Production Rule)이나 테이블 형태로 표현할 수 있다.

포상함수(Rt)는 각 행동과 그에 따르는 포상과의 관계를 나타낸다.

함수 V는 환경에서 각 상태가 차지하는 값을 계산해 준다. 따라서 포상함수는 계산하기 쉬우나(현재의 상태만 고려하면 되기 때문에) 상태의 값을 계산하는 것은 목표로 가는 과정에 변할 수 있는 상태 값들 까지도 고려해야 되므로 더욱 복잡하다.
